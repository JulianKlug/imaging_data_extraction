"""
given a perfusion_parameter_file generated by extract_perfusion_parameters.py,
verify it using a ground truth file. The columns of the ground truth file are "patient_id" "1st brain imaging date"	"T10" "T8" "T6" "T4" "CBF" "CTP_present"
T10 reprsents Tmax > 10s volume, T8 represents Tmax > 8s volume, T6 represents Tmax > 6s volume, and CBF represents relative CBF < 30% volume.
patients should be matched on these two columns: patient_id and 1st brain imaging date
example data are in temp_data
- example perfusion_parameter_file: temp_data/perfusion_parameters.csv
- example ground_truth_perfusion_parameters_file: temp_data/random_subset_for_imaging_extraction.xlsx
"""

import pandas as pd
import numpy as np
import argparse
import sys
import os

def test_automated_perfusion_parameter_extraction(automatically_extracted_perfusion_parameters_file, ground_truth_perfusion_parameters_file):
    """
    Test automatically extracted perfusion parameters against ground truth data.
    
    Args:
        automatically_extracted_perfusion_parameters_file: Path to CSV file with extracted parameters
        ground_truth_perfusion_parameters_file: Path to Excel/CSV file with ground truth data
        
    Returns:
        dict: Dictionary containing test results and metrics
    """
    
    # Load the automatically extracted perfusion parameters
    extracted_df = pd.read_csv(automatically_extracted_perfusion_parameters_file)
    
    # Load ground truth data (handle both Excel and CSV)
    if ground_truth_perfusion_parameters_file.endswith('.xlsx'):
        ground_truth_df = pd.read_excel(ground_truth_perfusion_parameters_file)
    else:
        ground_truth_df = pd.read_csv(ground_truth_perfusion_parameters_file)
    
    # Convert acquisition_date to datetime format (YYYYMMDD to YYYY-MM-DD)
    extracted_df['acquisition_date'] = pd.to_datetime(extracted_df['acquisition_date'], format='%Y%m%d')
    
    # Convert "1st brain imaging date" to datetime if it's not already
    if '1st brain imaging date' in ground_truth_df.columns:
        # Handle case where dates might be stored as integers (YYYYMMDD format)
        if ground_truth_df['1st brain imaging date'].dtype in ['int64', 'float64']:
            ground_truth_df['1st brain imaging date'] = pd.to_datetime(ground_truth_df['1st brain imaging date'], format='%Y%m%d')
        else:
            ground_truth_df['1st brain imaging date'] = pd.to_datetime(ground_truth_df['1st brain imaging date'])
    
    # Transform the extracted data to match ground truth format
    # Create a pivot table to get parameter values by patient and date
    extracted_pivot = extracted_df.pivot_table(
        index=['patient_id', 'acquisition_date'],
        columns=['parameter_type', 'threshold'],
        values='volume',
        aggfunc='first'
    ).reset_index()
    
    # Flatten the multi-level column names
    extracted_pivot.columns = ['_'.join([str(col) for col in cols]).strip('_') 
                              if isinstance(cols, tuple) else cols 
                              for cols in extracted_pivot.columns]
    
    # Map the extracted parameters to ground truth column names
    parameter_mapping = {
        'CBF_<30': 'CBF',
        'Tmax_>10.0': 'T10', 
        'Tmax_>8.0': 'T8',
        'Tmax_>6.0': 'T6',
        'Tmax_>4.0': 'T4'
    }
    
    # Create standardized extracted dataframe
    extracted_standardized = pd.DataFrame()
    extracted_standardized['patient_id'] = extracted_pivot['patient_id']
    extracted_standardized['1st brain imaging date'] = extracted_pivot['acquisition_date']
    
    # Map extracted parameters to ground truth format
    for extracted_col, gt_col in parameter_mapping.items():
        if extracted_col in extracted_pivot.columns:
            extracted_standardized[gt_col] = extracted_pivot[extracted_col]
        else:
            extracted_standardized[gt_col] = np.nan
    
    # Merge on patient_id and imaging date (inner join to only include studies found in both datasets)
    merged_df = pd.merge(
        ground_truth_df[['patient_id', '1st brain imaging date', 'T10', 'T8', 'T6', 'T4', 'CBF']],
        extracted_standardized,
        on=['patient_id', '1st brain imaging date'],
        how='inner',
        suffixes=('_gt', '_extracted')
    )
    
    print(f"Studies found in both datasets: {len(merged_df)}")
    
    if len(merged_df) == 0:
        print("Warning: No matching patient-date combinations found between datasets!")
        print("This could indicate:")
        print("- Different patient ID formats")
        print("- Different date formats") 
        print("- Datasets from different time periods")
        print("- Different data sources")
        return {}
    
    # Calculate comparison metrics
    results = {}
    parameters = ['T10', 'T8', 'T6', 'T4', 'CBF']
    
    # Create detailed comparison dataframe
    comparison_df = merged_df[['patient_id', '1st brain imaging date']].copy()
    
    for param in parameters:
        gt_col = f'{param}_gt'
        extracted_col = f'{param}_extracted'
        
        # Add ground truth and extracted columns to comparison dataframe
        comparison_df[f'{param}_ground_truth'] = merged_df[gt_col] if gt_col in merged_df.columns else np.nan
        comparison_df[f'{param}_extracted'] = merged_df[extracted_col] if extracted_col in merged_df.columns else np.nan
        
        if gt_col in merged_df.columns and extracted_col in merged_df.columns:
            # Remove rows where both values are NaN
            valid_rows = ~(merged_df[gt_col].isna() & merged_df[extracted_col].isna())
            gt_values = merged_df.loc[valid_rows, gt_col]
            extracted_values = merged_df.loc[valid_rows, extracted_col]
            
            # Fill NaN values with 0 for comparison (assuming missing means 0 volume)
            gt_values = gt_values.fillna(0)
            extracted_values = extracted_values.fillna(0)
            
            # Add exact match column
            exact_matches = (gt_values == extracted_values)
            comparison_df[f'{param}_exact_match'] = False
            comparison_df.loc[valid_rows, f'{param}_exact_match'] = exact_matches
            
            if len(gt_values) > 0:
                # Calculate metrics
                absolute_diff = np.abs(gt_values - extracted_values)
                relative_diff = np.where(gt_values != 0, 
                                       absolute_diff / gt_values * 100, 
                                       np.where(extracted_values == 0, 0, 100))
                
                # Correlation coefficient
                correlation = np.corrcoef(gt_values, extracted_values)[0, 1] if len(gt_values) > 1 else np.nan
                
                results[param] = {
                    'n_comparisons': len(gt_values),
                    'mean_absolute_error': np.mean(absolute_diff),
                    'median_absolute_error': np.median(absolute_diff),
                    'mean_relative_error_percent': np.mean(relative_diff),
                    'median_relative_error_percent': np.median(relative_diff),
                    'correlation': correlation,
                    'exact_matches': np.sum(gt_values == extracted_values),
                    'within_5ml': np.sum(absolute_diff <= 5),
                    'within_10ml': np.sum(absolute_diff <= 10),
                    'within_20_percent': np.sum(relative_diff <= 20)
                }
            else:
                # Add empty columns if no valid comparisons
                comparison_df[f'{param}_exact_match'] = False
                
                results[param] = {
                    'n_comparisons': 0,
                    'mean_absolute_error': np.nan,
                    'median_absolute_error': np.nan,
                    'mean_relative_error_percent': np.nan,
                    'median_relative_error_percent': np.nan,
                    'correlation': np.nan,
                    'exact_matches': 0,
                    'within_5ml': 0,
                    'within_10ml': 0,
                    'within_20_percent': 0
                }
        else:
            # Add empty columns if parameter not found
            comparison_df[f'{param}_exact_match'] = False
            
            results[param] = {
                'n_comparisons': 0,
                'mean_absolute_error': np.nan,
                'median_absolute_error': np.nan,
                'mean_relative_error_percent': np.nan,
                'median_relative_error_percent': np.nan,
                'correlation': np.nan,
                'exact_matches': 0,
                'within_5ml': 0,
                'within_10ml': 0,
                'within_20_percent': 0
            }
    
    # Overall summary
    total_comparisons = sum([results[param]['n_comparisons'] for param in parameters])
    total_ground_truth = len(ground_truth_df)
    total_extracted = len(extracted_standardized)
    matched_cases = len(merged_df.dropna(subset=['patient_id']))
    
    results['summary'] = {
        'total_ground_truth_cases': total_ground_truth,
        'total_extracted_cases': total_extracted,
        'matched_cases': matched_cases,
        'total_parameter_comparisons': total_comparisons,
        'coverage_rate': matched_cases / total_ground_truth if total_ground_truth > 0 else 0
    }
    
    # Add comparison dataframe to results
    results['comparison_dataframe'] = comparison_df
    
    # Print results summary
    print("=== Perfusion Parameter Extraction Test Results ===\n")
    print(f"Ground truth cases: {total_ground_truth}")
    print(f"Extracted cases: {total_extracted}")
    print(f"Matched cases: {matched_cases}")
    print(f"Coverage rate: {results['summary']['coverage_rate']:.2%}\n")
    
    for param in parameters:
        if results[param]['n_comparisons'] > 0:
            exact_match_percentage = results[param]['exact_matches'] / results[param]['n_comparisons'] * 100
            print(f"{param}: {exact_match_percentage:.1f}% exact matches ({results[param]['exact_matches']}/{results[param]['n_comparisons']} cases)")
        else:
            print(f"{param}: No valid comparisons available")
    
    return results


def main():
    """Main function with CLI argument parsing."""
    parser = argparse.ArgumentParser(
        description='Test automated perfusion parameter extraction against ground truth data',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  %(prog)s extracted_params.csv ground_truth.xlsx
  %(prog)s temp_data/perfusion_parameters.csv temp_data/random_subset_for_imaging_extraction.xlsx
        """
    )
    
    parser.add_argument(
        'extracted_file',
        help='Path to CSV file with automatically extracted perfusion parameters'
    )
    
    parser.add_argument(
        'ground_truth_file',
        help='Path to Excel/CSV file with ground truth perfusion parameters'
    )
    
    parser.add_argument(
        '--output', '-o',
        help='Optional path to save results as JSON file (default: test_results.json in extracted data directory)',
        default=None
    )
    
    parser.add_argument(
        '--csv-output', '-c',
        help='Optional path to save detailed comparison as CSV file (default: detailed_comparison.csv in extracted data directory)',
        default=None
    )
    
    parser.add_argument(
        '--quiet', '-q',
        action='store_true',
        help='Suppress detailed output, only show summary'
    )
    
    args = parser.parse_args()
    
    # Set default output paths in the directory of the extracted file
    extracted_dir = os.path.dirname(os.path.abspath(args.extracted_file))
    
    if args.output is None:
        args.output = os.path.join(extracted_dir, 'test_vs_ground_truth_results.json')
    
    if args.csv_output is None:
        args.csv_output = os.path.join(extracted_dir, 'test_comparison_with_ground_truth.csv')
    
    # Check if input files exist
    if not os.path.exists(args.extracted_file):
        print(f"Error: Extracted file '{args.extracted_file}' not found.", file=sys.stderr)
        sys.exit(1)
    
    if not os.path.exists(args.ground_truth_file):
        print(f"Error: Ground truth file '{args.ground_truth_file}' not found.", file=sys.stderr)
        sys.exit(1)
    
    try:
        # Run the test
        if not args.quiet:
            print(f"Testing extracted parameters from: {args.extracted_file}")
            print(f"Against ground truth from: {args.ground_truth_file}")
            print("-" * 60)
        
        results = test_automated_perfusion_parameter_extraction(
            args.extracted_file, 
            args.ground_truth_file
        )
        
        # Save results to file (always save to default location)
        import json
        # Remove dataframe from results for JSON export (not JSON serializable)
        results_for_json = {k: v for k, v in results.items() if k != 'comparison_dataframe'}
        with open(args.output, 'w') as f:
            json.dump(results_for_json, f, indent=2, default=str)
        print(f"\nResults saved to: {args.output}")
        
        # Save comparison dataframe to CSV (always save to default location)
        results['comparison_dataframe'].to_csv(args.csv_output, index=False)
        print(f"Detailed comparison saved to: {args.csv_output}")
        
        return results
        
    except Exception as e:
        print(f"Error during analysis: {e}", file=sys.stderr)
        sys.exit(1)


if __name__ == "__main__":
    main()

